%! TEX root = ./main.tex
\begin{exercise}[]{Many good experts}
	Sequences of outcomes on which many experts suffer a small loss are intuitively easier to predict. Adapt the proof of Theorem 2.2 to show that the exponentially weighted forecaster satisfies the following property : for every $ n $, for every outcome sequence $ y^{n} $ and for all $ L >0 $,
\begin{equation*}
	\widehat{L_n} \leq L + \frac{1}{\eta}\log \frac{N}{N_L} + \frac{\eta}{8}n,
\end{equation*}
where $ N_L $ is the cardinality of the set $ \{ 1 \leq i \leq N : L_{i,n} \leq L \} $
\end{exercise}

\begin{solution}[]
	We adapt the lower bound of $ \log \frac{W_n}{W_0} $. We have :
\begin{align*}
	\log \frac{W_n}{W_0} &= \log \left( \sum_{i=1}^{N}e^{-\eta L_{i,n}} \right) - \log N \\
			     &\geq \log \left( \sum_{i=1}^{N}e^{-\eta L_{i,n}}\cdot \mathbb{I}_{L_{i,n}\leq L} \right) - \log N \\
			     &\geq \log \left( \sum_{i=1}^{N}e^{-\eta L}\cdot \mathbb{I}_{L_{i,n}\leq L} \right) - \log N \\
			     &=\log \left( N_L e^{-\eta L} \right) - \log N \\
			     &= - \eta L - \log \frac{N}{N_L}.
\end{align*}
We still have the upper bound of $ \log \frac{W_n}{W_0} $ from the proof of Theorem 2.2 :
\begin{equation*}
	\log \frac{W_n}{W_0} \leq -\eta \widehat{L_n} + \frac{\eta^2}{8}n.
\end{equation*}
Putting everything together, we get :
\begin{equation*}
	\widehat{L_n} \leq L + \frac{1}{\eta}\log \frac{N}{N_L} + \frac{\eta}{8}n,
\end{equation*}


\end{solution}
