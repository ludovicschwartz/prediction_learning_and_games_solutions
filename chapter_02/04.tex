%! TEX root = ./main.tex
\begin{exercise}[]{}
	Let $ \mathcal{Y} = \{ 0,1 \}, \mathcal{D} = [0,1] $, and $ \ell(\bar{p},y) = |\bar{p}-y| $. Prove that the cumulative loss $ \hat{L} $ of the exponentially weighted aerage forecaster is always at least as largeas the cumulative loss $ \min_{i \leq N}L_i $ of the best expert. Show that for other loss functions, such as the square loss $ (\bar{p}-y)^2 $, this is not necessarily so. \textit{Hint :} Try to reverse the proof of Theorem 2.2. 
\end{exercise}

\begin{solution}[]
	As suggested by the hint, we will adapt the proof of Theorem 2.2. Using the same notations, we observe that 
\begin{align*}
	\log \frac{W_n}{W_0} &= \log\left(\sum_{i=1}^{N}e^{-\eta L_{i,n}}\right) - \log N \\
	&\leq \log \left( N \max_{i=1,\ldots,N}e^{-\eta L_{i,n}} \right) - \log N\\
	&= - \eta \min_{i=1,\ldots,N} L_{i,n}.
\end{align*}
Here rather than lower bounding the sum by its biggest element, we upper bound it by the number of elements time the maximal elements. Now we will obtain a lower bound of $ \log \frac{W_n}{W_0} $. We have for each $ t=1,\ldots,n, $
\begin{align*}
	\log \frac{W_t}{W_{t-1}} &= \log \frac{\sum_{i=1}^{N}e^{- \eta \ell(f_{i,t},y_t)e^{-\eta L_{i,t-1}}}}{\sum_{i=1}^{N}e^{-\eta L_{i,t-1}}}\\
				 &= \log \frac{\sum_{i=1}^{N}w_{i,t-1}e^{-\eta\ell(f_{i,t},y_t)}}{\sum_{i=1}^{T}w_{i,t-1}}.
\end{align*}
Now we define $ \bar{w}_{i,t} = \frac{w_{i,t}}{\sum_{j=1}^{N}w_{j,t}} $ the normalized exponential weights. We have :
\begin{align*}
	\log \frac{W_t}{W_{t-1}} &= \log \sum_{i=1}^{N} \bar{w}_{i ,t-1}\cdot e^{-\eta\ell(f_{i ,t},y_t)}\\
				 &\leq \sum_{i=1}^{N}\bar{w}_{i ,t-1}\cdot (-\eta\ell(f_{i ,t},y_t)),
\end{align*}
where we use the concavity of the logarithm and Jensen's Inequality in the second line. Now, if $ y_t = 0 $, for any $ f\in \mathcal{D} $ we have $ \ell(f, y_t) = f $, and if $ y_t =1 $, $ \ell(f,y_t) = 1-f $. In particular, the loss function is linear in it's first coordinate. In particular,
\begin{equation*}
	\log \frac{W_t}{W_{t-1}} = -\eta \ell(\sum_{i=1}^{N}\bar{w}_{i,t-1}f_{i.t},y_t) = -\eta \ell(\hat{p}_t,y_t).
\end{equation*}
Here we used the absolute loss and the particular shape of the decision and outcome spaces. This particular steps is not possible anymore with the square loss.
Summing over $ t=1,\ldots,n $ and combining both bounds, we get :
\begin{equation*}
	\hat{L}_t = \sum_{t=1}^{n}\ell(\hat{p}_t,y_t) \geq \frac{-1}{\eta}\log \frac{W_n}{W_0} \geq \min_{i=1,\ldots,N} L_{i,n}.
\end{equation*}
For a counterexample with the square loss, we consider two experts, the first one always predict 0, and the second one always predicts 1. We then choose $ y_t $ to be 0 on even rounds and 1 on odd rounds. We run the algorithm during $ 2n $ rounds. It is easy to see that the regret of both experts is n(They suffer a regret of 1 during exactly half of the rounds). During odd rounds, our learner puts the same weight on both experts and suffers a regret of $ \frac{1}{4} $. During even rounds, our algorithm makes the prediction $ \frac{e^{\eta}}{e^{\eta} + e^{-\eta}} $. So the exponential weighted forecaster has a regret of $ n(\frac{1}{4} + (\frac{e^{\eta}}{e^{\eta} + e^{-\eta}})^2) $. This regret is smaller than the one of either experts as long as $ \eta $ is small enough which will be true for the optimal $ \eta^{*} \propto \frac{1}{\sqrt{n}} $ for n big enough.

\end{solution}
